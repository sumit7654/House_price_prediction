


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split



train_data_path= r"E:\Machine_learnign_projects\House_price_prediction\data\train.csv"
test_data_path = r"E:\Machine_learnign_projects\House_price_prediction\data\test.csv"

df_train = pd.read_csv(train_data_path)
df_test = pd.read_csv(test_data_path)

print("Shape of train data" , df_train.shape)
print("Shape of test data " , df_test.shape)


pd.set_option("display.max_columns" , None)
pd.set_option("display.max_rows" , None)


df_train.head()


df_train.info()


df_train.describe()


# features = ["OverallQual", "YearBuilt", "GarageCars", "TotalBsmtSF" , "OverallCond" , "GrLivArea" , "BedroomAbvGr" , "KitchenAbvGr"]
features = [
    "OverallQual", "GrLivArea", "GarageCars", "TotalBsmtSF",
    "YearBuilt", "1stFlrSF", "FullBath", "TotRmsAbvGrd",
    "KitchenQual", "Neighborhood", "ExterQual", "BsmtQual",
    "HeatingQC", "SaleCondition"
]
data_clean = df_train[df_train['GrLivArea'] < 4500]
X= data_clean[features]
y = np.log1p(data_clean["SalePrice"])
X = pd.get_dummies(X,drop_first=True)
X = X.fillna(X.median())


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)


X = X.fillna(X.median())



from sklearn.metrics import mean_squared_error


y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5

print("RMSE:", rmse)


from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = mse_rf ** 0.5
print("Random Forest RMSE:", rmse_rf)



from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor(n_estimators=300, max_depth=20, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
rmse = (mean_squared_error(y_test, y_pred)) ** 0.5
print("Improved Random Forest RMSE:", rmse)






from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

model_xgb = XGBRegressor(
    n_estimators=900,
    learning_rate=0.03,
    max_depth=4,
    subsample=0.85,
    colsample_bytree=0.85,
    random_state=42
)

model_xgb.fit(X_train, y_train)
y_pred = model_xgb.predict(X_test)

rmse = (mean_squared_error(np.expm1(y_test), np.expm1(y_pred))) ** 0.5
print("XGBoost (Log + Outlier Removal) RMSE:", rmse)







